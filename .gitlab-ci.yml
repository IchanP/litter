stages:
  - setup
  - test
  - build
  - deploy

variables:
  STAGING_CONTEXT: $STAGING_AGENT
  PROD_CONTEXT: $PROD_AGENT

.publish_image_staging: &publish_image_staging |
  function publish_service_staging() {
    local service_path=$1
    /kaniko/executor \
      --context "${CI_PROJECT_DIR}/${service_path}" \
      --dockerfile "${CI_PROJECT_DIR}/${service_path}/Dockerfile.production" \
      --destination "${CI_REGISTRY_IMAGE}/${service_path}:staging-${CI_COMMIT_SHA}" \
      --destination "${CI_REGISTRY_IMAGE}/${service_path}:${CI_COMMIT_TAG:-staging-latest}"
  }

.publish_image_production: &publish_image_production |
  function publish_service_production() {
    local service_path=$1
    /kaniko/executor \
      --context "${CI_PROJECT_DIR}/${service_path}" \
      --dockerfile "${CI_PROJECT_DIR}/${service_path}/Dockerfile.production" \
      --destination "${CI_REGISTRY_IMAGE}/${service_path}:${CI_COMMIT_SHA}" \
      --destination "${CI_REGISTRY_IMAGE}/${service_path}:${CI_COMMIT_TAG:-latest}"
  }

.build_image: &build_image |
  function build_image() {
    local service_path=$1
    /kaniko/executor \
     --context "${CI_PROJECT_DIR}/${service_path}" \
     --dockerfile "${CI_PROJECT_DIR}/${service_path}/Dockerfile.production" \
     --no-push
  }

setup_kubectl_dependencies:
  stage: setup
  image: google/cloud-sdk:latest  # This comes with kubectl, curl, and other tools
  script:
    - echo "Current context:"
    - kubectl config get-contexts
    - apt-get update && apt-get install -y gettext-base
    
    - |
      if [[ $CI_COMMIT_BRANCH == "main" ]]; then
        export K8S_CONTEXT=$PROD_CONTEXT
      else
        export K8S_CONTEXT=$STAGING_CONTEXT
      fi
    # Set the context
    - kubectl config use-context $K8S_CONTEXT
    - echo "After setting context:"
    - kubectl config current-context

    # Apply shared secrets
    - envsubst < shared-secrets/docker-registry.yaml | kubectl apply -f -
    - envsubst < shared-secrets/topic-names.yaml | kubectl replace --force -f -
    - envsubst < shared-secrets/service-dns.yaml | kubectl replace --force -f -
    - envsubsts < shared-secrets/kafka-connection.yaml | kubectl replace --force -f -
    # Apply operator
    - |
        if ! kubectl get deployment strimzi-cluster-operator -n default &> /dev/null; then
          echo "Strimzi operator not found. Installing..."
          curl -L https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.31.1/strimzi-cluster-operator-0.31.1.yaml | sed 's/namespace: myproject/namespace: default/g' | kubectl apply -f -
          kubectl wait --for=condition=available --timeout=300s deployment/strimzi-cluster-operator -n default
        else
          echo "strimzi operator already exists"
        fi

write_service_tests:
  stage: test
  image:
    name: node:20-slim
  script:
   - echo "Running write-service tests"
   - cd write-service
   - npm ci
   - npm run test


setup_staging_secrets:
  stage: setup
  image:
    name: bitnami/kubectl:latest
    entrypoint: [""]
  variables:
    K8S_CONTEXT: $STAGING_CONTEXT
  script:
    - kubectl config use-context $K8S_CONTEXT
    - envsubst < ./frontend/k8s/secrets/auth0-litter-staging.yaml | kubectl apply -f -
    # TODO - Remove this secret?
    - envsubst < ./api-gateway/k8s/secrets/jwt-secret-staging.yaml | kubectl apply -f -
  needs:
    - setup_kubectl_dependencies
  only:
    - staging
    - pipeline/staging

setup_production_secrets:
  stage: setup
  image:
    name: bitnami/kubectl:latest
    entrypoint: [""]
  variables:
    K8S_CONTEXT: $PROD_CONTEXT
  script:
    - kubectl config use-context $K8S_CONTEXT
    - envsubst < ./frontend/k8s/secrets/auth0-litter-prod.yaml | kubectl apply -f -
    # TODO - Remove this secret?
    - envsubst < ./api-gateway/k8s/secrets/jwt-secret.yaml | kubectl apply -f -
  needs:
    - setup_kubectl_dependencies
  only:
    - main

build:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:v1.9.0-debug
    entrypoint: [""]
  script:
   - echo "Building Image"
   - *build_image
   - build_image "frontend"
   - build_image "write-service"
   - build_image "api-gateway"
   - build_image "post-service"
   - build_image "user-service"

publish_staging:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:v1.9.0-debug
    entrypoint: [""]
  script:
    - echo "Rebuilding and publishing images for staging"

    - *publish_image_staging
    - publish_service_staging "frontend"
    - publish_service_staging "write-service"
    - publish_service_staging "api-gateway"
    - publish_service_staging "post-service"
    - publish_service_staging "user-service"

  only:
    - staging
    - pipeline/staging
  needs:
    - build

publish_production:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:v1.9.0-debug
    entrypoint: [""]
  script:
    - echo "Rebuilding and publishing images for production"
    - *publish_image_production
    - publish_service_production "frontend"
    - publish_service_production "write-service"
    - publish_service_production "api-gateway"
    - publish_service_production "post-service"
    - publish_service_production "user-service"

  only:
    - main
  needs:
   - build

deploy_production:
  stage: deploy
  image:
    name: gcr.io/k8s-skaffold/skaffold:v2.8.0
    entrypoint: [""]
  variables:
    K8S_CONTEXT: $PROD_CONTEXT
  environment:
    name: production
    url: $PROD_URL
  when: manual
  script:
    - kubectl config get-contexts
    - kubectl config use-context $K8S_CONTEXT

    - kubectl delete secret regcred || true
    - |
        kubectl create secret docker-registry regcred \
          --docker-server=$CI_REGISTRY \
          --docker-username=$CI_REGISTRY_USER \
          --docker-password=$CI_REGISTRY_PASSWORD

    # Use the images created from the publish_production step
    # Also the image names and paths need to be replaced

    - |
      skaffold deploy --profile production -v debug \
      --images frontend=${CI_REGISTRY_IMAGE}/frontend:${CI_COMMIT_SHA} \
      --images write-service=${CI_REGISTRY_IMAGE}/write-service:${CI_COMMIT_SHA} \
      --images api-gateway=${CI_REGISTRY_IMAGE}/api-gateway:${CI_COMMIT_SHA} \
      --images user-service=${CI_REGISTRY_IMAGE}/user-service:${CI_COMMIT_SHA} \
      --images post-service=${CI_REGISTRY_IMAGE}/post-service:${CI_COMMIT_SHA}

    # Debugging
    - kubectl get deployments
    - kubectl get pods
    - kubectl get services
  only:
    - main

deploy_staging:
  stage: deploy
  image:
    name: gcr.io/k8s-skaffold/skaffold:v2.8.0
    entrypoint: [""]
  variables:
    K8S_CONTEXT: $STAGING_CONTEXT
  environment:
    name: staging
    url: $STAGING_URL
  script:
    - kubectl config get-contexts
    - kubectl config use-context $K8S_CONTEXT
  
    - kubectl delete secret regcred || true
    - |
        kubectl create secret docker-registry regcred \
          --docker-server=$CI_REGISTRY \
          --docker-username=$CI_REGISTRY_USER  \
          --docker-password=$CI_REGISTRY_PASSWORD \
          --docker-email=$DOCKER_EMAIL
  
    # Use the images created from the publish_production step
    # TODO how do I make this deploy only to namespace staging? Maybe need to do that in skaffold/yaml config...
    # Also the image names and paths need to be replaced
    - |
      skaffold deploy --profile production \
        --images frontend=${CI_REGISTRY_IMAGE}/frontend:staging-${CI_COMMIT_SHA} \
        --images write-service=${CI_REGISTRY_IMAGE}/write-service:staging-${CI_COMMIT_SHA} \
        --images api-gateway=${CI_REGISTRY_IMAGE}/api-gateway:staging-${CI_COMMIT_SHA} \
        --images user-service=${CI_REGISTRY_IMAGE}/user-service:staging-${CI_COMMIT_SHA} \
        --images post-service=${CI_REGISTRY_IMAGE}/post-service:staging-${CI_COMMIT_SHA}
  
    # Debugging
    - kubectl get deployments
    - kubectl get pods
    - kubectl get services
  only:
    - staging
    - pipeline/staging